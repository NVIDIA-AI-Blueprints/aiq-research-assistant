# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Artifact uploader that runs as a special evaluator to upload results to Weave.
This integrates with the AIQ evaluation framework and automatically uses the same
Weave project as the tracing configuration.
"""

import json
import logging
import time
import asyncio
from pathlib import Path
from typing import Any, Dict, List, Optional
from datetime import datetime

from pydantic import Field

from aiq.builder.builder import EvalBuilder
from aiq.builder.evaluator import EvaluatorInfo
from aiq.cli.register_workflow import register_evaluator
from aiq.data_models.evaluator import EvaluatorBaseConfig
from aiq.eval.evaluator.evaluator_model import EvalOutputItem, EvalInput, EvalOutput

logger = logging.getLogger(__name__)


class ArtifactUploaderConfig(EvaluatorBaseConfig, name="artifact_uploader"):
    """Configuration for the artifact uploader."""
    enabled: bool = Field(default=True, description="Whether to upload artifacts to Weave")
    project_name: Optional[str] = Field(default=None, description="Project name for artifact upload. If None, will try to detect from Weave context")
    run_name: Optional[str] = Field(default=None, description="Custom run name for artifact upload. If None, will auto-generate")
    wait_for_fresh_results: bool = Field(default=True, description="Wait for fresh evaluation results before uploading")
    max_wait_time: int = Field(default=300, description="Maximum time to wait for fresh results (seconds)")
    

class ArtifactUploader:
    """
    Special evaluator that uploads all evaluation results as Weave artifacts.
    This runs after all other evaluators and uses the same project as Weave tracing.
    """
    
    def __init__(self, enabled: bool = True, max_concurrency: int = 1, project_name: Optional[str] = None, 
                 run_name: Optional[str] = None, wait_for_fresh_results: bool = True, max_wait_time: int = 300):
        self.enabled = enabled
        self.max_concurrency = max_concurrency
        self.project_name = project_name
        self.run_name = run_name
        self.wait_for_fresh_results = wait_for_fresh_results
        self.max_wait_time = max_wait_time
        self.start_time = time.time()
        
    async def evaluate(self, eval_input: EvalInput) -> EvalOutput:
        """
        Upload artifacts to Weave. This doesn't actually evaluate anything,
        but uses the evaluator framework to run after all other evaluators.
        """
        if not self.enabled:
            logger.info("Artifact upload disabled")
            return self._create_dummy_output(eval_input)
        
        try:
            # Wait for fresh results if enabled
            if self.wait_for_fresh_results:
                await self._wait_for_fresh_results()
            
            # Get the Weave project from the current initialization
            weave_project = self._get_weave_project()
            
            # Upload artifacts
            await self._upload_artifacts(weave_project)
            
            logger.info(" Successfully uploaded artifacts to Weave")
            return self._create_success_output(eval_input)
            
        except Exception as e:
            logger.error(f"Failed to upload artifacts: {e}")
            return self._create_error_output(eval_input, str(e))
    
    async def _wait_for_fresh_results(self) -> None:
        """Wait for evaluation results to be generated by other evaluators."""
        results_dir = Path(".tmp/aiq_aira")
        evaluator_patterns = [
            "*coverage_output.json",
            "*synthesis_output.json", 
            "*hallucination_output.json",
            "*citation_quality_output.json",
            "*rag_*_output.json"
        ]
        
        logger.info("Waiting for fresh evaluation results...")
        wait_start = time.time()
        
        while time.time() - wait_start < self.max_wait_time:
            # Check if we have fresh result files (modified after this evaluator started)
            fresh_files = []
            
            if results_dir.exists():
                for pattern in evaluator_patterns:
                    for file_path in results_dir.glob(pattern):
                        if file_path.stat().st_mtime > self.start_time:
                            fresh_files.append(file_path.name)
            
            if fresh_files:
                logger.info(f"Found {len(fresh_files)} fresh result files: {fresh_files}")
                # Wait a bit more to ensure all evaluators finish writing
                await asyncio.sleep(5)
                return
            
            # Check every 2 seconds
            await asyncio.sleep(2)
        
        logger.warning(f"Timeout waiting for fresh results after {self.max_wait_time}s. Proceeding with available files.")
    
    def _get_weave_project(self) -> Optional[str]:
        """Get the Weave project name from the current Weave initialization."""
        try:
            import weave
            
            # Try to get the current project from Weave's context
            if hasattr(weave, 'get_current_call'):
                current_call = weave.get_current_call()
                if current_call and hasattr(current_call, 'project_id'):
                    return current_call.project_id
            
            # Fallback: try to get from weave client
            if hasattr(weave, 'client') and weave.client:
                return getattr(weave.client, 'project', None)
                
            # Another fallback: check environment or global state
            if hasattr(weave, '_current_project'):
                return weave._current_project
                
            return None
            
        except ImportError:
            logger.warning("Weave not available")
            return None
        except Exception as e:
            logger.warning(f"Could not determine Weave project: {e}")
            return None
    
    async def _upload_artifacts(self, weave_project) -> None:
        """Upload evaluation artifacts to Weave."""
        results_dir = Path(".tmp/aiq_aira")
        
        if not results_dir.exists():
            logger.warning(f"Results directory {results_dir} does not exist")
            return
        
        # Find all evaluation output files
        output_files = list(results_dir.glob("*_output.json"))
        
        if not output_files:
            logger.warning(f"No evaluation output files found in {results_dir}")
            return
        
        logger.info(f"Found {len(output_files)} evaluation output files to upload")
        
        for output_file in output_files:
            try:
                # Extract evaluator name from filename (e.g., "coverage_output.json" -> "coverage")
                evaluator_name = output_file.stem.replace("_output", "")
                
                # Read the evaluation results
                with open(output_file, 'r') as f:
                    eval_data = json.load(f)
                
                # Create artifact name with timestamp
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                artifact_name = f"eval_{evaluator_name}_{timestamp}"
                
                # Upload as Weave artifact
                artifact = weave.Artifact(
                    name=artifact_name,
                    type="evaluation_results",
                    description=f"Evaluation results from {evaluator_name} evaluator",
                    metadata={
                        "evaluator": evaluator_name,
                        "timestamp": timestamp,
                        "file_path": str(output_file),
                        "num_items": len(eval_data.get("eval_output_items", [])),
                        "average_score": eval_data.get("average_score", 0.0)
                    }
                )
                
                # Set the artifact data
                artifact.data = eval_data
                
                # Publish to Weave
                weave_project.save(artifact)
                
                logger.info(f"âœ“ Uploaded {evaluator_name} results as artifact '{artifact_name}'")
                
            except Exception as e:
                logger.error(f"Failed to upload {output_file}: {e}")
                continue
    
    def _create_dummy_output(self, eval_input: EvalInput) -> EvalOutput:
        """Create a dummy output when artifact upload is disabled."""
        items = [
            EvalOutputItem(
                id=item.id,
                score=None,
                reasoning={"message": "Artifact upload disabled"}
            )
            for item in eval_input.eval_input_items
        ]
        
        return EvalOutput(
            average_score=0.0,
            eval_output_items=items
        )
    
    def _create_success_output(self, eval_input: EvalInput) -> EvalOutput:
        """Create a success output after successful artifact upload."""
        items = [
            EvalOutputItem(
                id=item.id,
                score=1.0,
                reasoning={"message": "Artifacts successfully uploaded to Weave"}
            )
            for item in eval_input.eval_input_items
        ]
        
        return EvalOutput(
            average_score=1.0,
            eval_output_items=items
        )
    
    def _create_error_output(self, eval_input: EvalInput, error_message: str) -> EvalOutput:
        """Create an error output when artifact upload fails."""
        items = [
            EvalOutputItem(
                id=item.id,
                score=0.0,
                reasoning={"message": f"Artifact upload failed: {error_message}"}
            )
            for item in eval_input.eval_input_items
        ]
        
        return EvalOutput(
            average_score=0.0,
            eval_output_items=items
        )


@register_evaluator(config_type=ArtifactUploaderConfig)
async def register_artifact_uploader(config: ArtifactUploaderConfig, builder: EvalBuilder):
    """Register the artifact uploader as a special evaluator."""
    evaluator = ArtifactUploader(
        enabled=config.enabled,
        max_concurrency=1,  # Always run with concurrency 1 since it's a singleton operation
        project_name=config.project_name,
        run_name=config.run_name,
        wait_for_fresh_results=config.wait_for_fresh_results,
        max_wait_time=config.max_wait_time
    )
    
    yield EvaluatorInfo(
        config=config,
        evaluate_fn=evaluator.evaluate,
        description="Artifact Uploader - Uploads evaluation results to Weave"
    ) 