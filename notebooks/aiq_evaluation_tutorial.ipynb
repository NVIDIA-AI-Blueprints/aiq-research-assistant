{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Q Research Assistant Evaluation Suite Tutorial\n",
        "\n",
        "This notebook provides a comprehensive walkthrough of the AI-Q Research Assistant Evaluation Suite, demonstrating how to evaluate AI-generated research reports with automatic dataset preprocessing and comprehensive quality metrics.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to set up the AI-Q Research Assistant evaluation framework\n",
        "- Creating and preprocessing evaluation datasets  \n",
        "- Configuring and running evaluations\n",
        "- Understanding evaluation metrics and results\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- Local Deployment of AI-Q Research Assistant or hosted endpoint\n",
        "    - Reference for setup lives at: docs/get-started/get-started-docker-compose.md\n",
        "- Local Deployment of Foundational RAG or hosted endpoint\n",
        "    - Steps for RAG deployment are also in the same deployment file at: docs/get-started/get-started-docker-compose.md\n",
        "- Default collections loaded, especially “Biomedical_Dataset” (see docs/get-started/get-started-docker-compose.md).\n",
        "    - This is required because both the evaluation suite and the research workflow read from this RAG collection.\n",
        "- Alternatively, deploy via Helm: deploy/helm (see docs/get-started/helm-deployment.md)\n",
        "- Python 3.12+\n",
        "- NVIDIA API Key (from [build.nvidia.com](https://build.nvidia.com))\n",
        "- (Optional) Tavily API Key for web search capabilities\n",
        "- (Optional) Wandb API Key for tracing capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Check Python version\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify we're in the right directory structure (notebook should be in notebooks/ subdirectory)\n",
        "if not Path(\"../pyproject.toml\").exists():\n",
        "    print(\"Please ensure this notebook is in the notebooks/ directory of the repository\")\n",
        "    print(\"    The '../pyproject.toml' file should be accessible from here.\")\n",
        "    print(\"    Current structure should be: repository_root/notebooks/this_notebook.ipynb\")\n",
        "else:\n",
        "    print(\"Directory structure verified - pyproject.toml found in parent directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install AI-Q Research Assistant package directly into the current Python environment\n",
        "print(\"Installing directly into current Python environment...\")\n",
        "print(f\"Using Python: {sys.executable}\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install directly using the current Python interpreter\n",
        "result = subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"..\"\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "print(f\"\\nInstallation result:\")\n",
        "if result.returncode == 0:\n",
        "    print(\"Installation successful!\")\n",
        "    if result.stdout:\n",
        "        print(\"Output:\", result.stdout.strip())\n",
        "else:\n",
        "    print(\"Installation failed!\")\n",
        "    print(\"Error:\", result.stderr)\n",
        "\n",
        "# Test import immediately\n",
        "print(f\"\\nTesting import...\")\n",
        "try:\n",
        "    # Clear any cached modules to get fresh import\n",
        "    modules_to_clear = [m for m in sys.modules.keys() if m.startswith('aiq')]\n",
        "    for module in modules_to_clear:\n",
        "        del sys.modules[module]\n",
        "    \n",
        "    import aiq_aira\n",
        "    print(\"SUCCESS: aiq_aira imported!\")\n",
        "    \n",
        "    \n",
        "    print(f\"\\nREADY TO GO!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"Import still failing: {e}\")\n",
        "    print(\"\\nFallback: Using manual path fix...\")\n",
        "    \n",
        "    # Fallback to manual path\n",
        "    aira_src_path = str(Path(\"../aira/src\").resolve())\n",
        "    if aira_src_path not in sys.path:\n",
        "        sys.path.insert(0, aira_src_path)\n",
        "        \n",
        "    try:\n",
        "        import aiq_aira\n",
        "        print(\"SUCCESS with manual path fix!\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"Still failing: {e2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Your API Keys\n",
        "\n",
        "Before running the evaluation, you need to set your NVIDIA API key. This is **required** for the evaluation to work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to set up the environment variables, you can also set them manually in your terminal\n",
        "# It will prompt you to enter your API key for NVIDIA\n",
        "\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "def ensure_secret(var_name: str, prompt_text: str, require_prefix: str | None = None) -> str:\n",
        "    val = os.environ.get(var_name, \"\").strip()\n",
        "    while True:\n",
        "        if not val:\n",
        "            val = getpass.getpass(prompt_text).strip()\n",
        "        if not val:\n",
        "            print(f\"{var_name} cannot be empty. Please try again.\")\n",
        "            continue\n",
        "        if require_prefix and not val.startswith(require_prefix):\n",
        "            print(f\"{var_name} should start with '{require_prefix}'. Please try again.\")\n",
        "            val = \"\"  # force re-prompt\n",
        "            continue\n",
        "        break\n",
        "    os.environ[var_name] = val\n",
        "    return val\n",
        "\n",
        "# Use it\n",
        "nvidia_key = ensure_secret(\n",
        "    \"NVIDIA_API_KEY\",\n",
        "    \"Please input your NVIDIA_API_KEY: \",\n",
        "    require_prefix=\"nvapi-\"\n",
        ")\n",
        "print(\"NVIDIA_API_KEY is set.\")\n",
        "\n",
        "key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
        "shown = f\"{key[:6]}...{key[-4:]}\" if len(key) >= 10 else \"***\"\n",
        "print(f\"NVIDIA_API_KEY (masked): {shown}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Setting up Web Search with Tavily"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To set up web search with tavily, you need to get an API key from tavily and set it in the environment variable TAVILY_API_KEY by running this cell\n",
        "# To enable web search during evaluation, please set `\"search_web\": true` in your dataset entries. See example in data/eval_dataset.json\n",
        "tavily = os.environ.get(\"TAVILY_API_KEY\", \"\").strip()\n",
        "if not tavily:\n",
        "    tmp = getpass.getpass(\"Optional: TAVILY_API_KEY (press Enter to skip): \").strip()\n",
        "    if tmp:\n",
        "        os.environ[\"TAVILY_API_KEY\"] = tmp\n",
        "        print(\"TAVILY_API_KEY is set (optional).\")\n",
        "    else:\n",
        "        print(\"TAVILY_API_KEY not set (optional).\")\n",
        "else:\n",
        "    print(\"TAVILY_API_KEY is set (optional).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: Setting up Tracing with W&B Weave\n",
        "\n",
        "- To turn tracing with weave go into /configs/eval_config.yml go under telemetry portion\n",
        "  - It is currently set to being off by default but uncommenting the portion below turns on weave tracing during evaluation\n",
        "\n",
        "```python\n",
        " telemetry:\n",
        "    logging:\n",
        "      console:\n",
        "        _type: console\n",
        "        level: DEBUG\n",
        "    # Uncomment this if you want to use W&B Weave for tracing\n",
        "    # tracing:\n",
        "    #   weave:\n",
        "    #     _type: weave\n",
        "    #     project: \"NAT-BP-Project-Default\" # Name of the project in weave, runs will be grouped under this project name\n",
        "```\n",
        "\n",
        "To view your results after your evaluation run head over to \n",
        "- https://wandb.ai/home\n",
        "\n",
        "For more documentation on how to set up Weave in Nemo Agent Toolkit workflow reference here:\n",
        "- https://docs.nvidia.com/nemo/agent-toolkit/1.2/workflows/observe/observe-workflow-with-weave.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Enable Weights & Biases (W&B) for Weave/tracing and experiment tracking.\n",
        "# Run this cell only if enabling W&B/Weave as it will set your WANDB_KEY\n",
        "\n",
        "import os, getpass\n",
        "\n",
        "def ensure_wandb_key() -> str:\n",
        "    key = os.environ.get(\"WANDB_API_KEY\", \"\").strip()\n",
        "    while not key:\n",
        "        key = getpass.getpass(\"Enter WANDB_API_KEY (or press Enter to cancel): \").strip()\n",
        "        if not key:\n",
        "            print(\"W&B setup skipped. Leave this cell unrun or set WANDB_API_KEY later to enable tracing.\")\n",
        "            return \"\"\n",
        "    os.environ[\"WANDB_API_KEY\"] = key\n",
        "    shown = f\"{key[:4]}...{key[-4:]}\" if len(key) >= 10 else \"***\"\n",
        "    print(f\"WANDB_API_KEY set (masked): {shown}\")\n",
        "    return key\n",
        "\n",
        "try:\n",
        "    key = ensure_wandb_key()\n",
        "    if key:\n",
        "        import wandb\n",
        "        base_url = os.environ.get(\"WANDB_BASE_URL\", \"\").strip()\n",
        "        if base_url:\n",
        "            os.environ[\"WANDB_BASE_URL\"] = base_url\n",
        "            wandb.login(key=key, host=base_url, verify=True, relogin=True)\n",
        "        else:\n",
        "            wandb.login(key=key, verify=True, relogin=True)\n",
        "        print(\"Weights & Biases login successful. W&B is enabled for this session.\")\n",
        "except Exception as e:\n",
        "    print(\"Failed to log in to Weights & Biases. Check WANDB_API_KEY or connectivity.\")\n",
        "    print(str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Quick Start - Run a Basic Evaluation\n",
        "\n",
        "Let's run a quick evaluation to test everything is working. We'll use the default dataset and configuration included in the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Set Environment variable overrides for service endpoints\n",
        "\n",
        "import os\n",
        "\n",
        "# Default: self-hosted/local NIMs\n",
        "# Hosted Instruct LLM Backend (set to meta/llama-3.3-70b-instruct)\n",
        "os.environ[\"INSTRUCT_LLM_BASE_URL\"]=\"http://localhost:8050/v1}\"\n",
        "# Hosted Nemotron Backend (for reasoning)\n",
        "os.environ[\"NEMOTRON_LLM_BASE_URL\"]=\"http://localhost:8999/v1\"\n",
        "# RAG server for generate_summary / artifact_qa\n",
        "os.environ[\"RAG_SERVER_URL\"]=\"http://localhost:8081/v1\"\n",
        "\n",
        "# Optional: If you want to use hosted endpoints for the evaluation LLM and RAGAS LLM, uncomment the following lines and point to the correct hosted endpoints\n",
        "# Else, the default for evaluation LLM and RAGAS LLM are to use models hosted on NVIDIA's build.nvidia.com\n",
        "# os.environ[\"EVAL_LLM_BASE_URL\"]=\"http://local-llm-endpoint:8000/v1\"\n",
        "# os.environ[\"RAGAS_LLM_BASE_URL\"]=\"http://local-llm-endpoint:8000/v1\"\n",
        "\n",
        "\n",
        "# NVIDIA hosted example: uncomment ALL lines below to use hosted endpoints, you must have your own RAG server and set the correct endpoints\n",
        "# os.environ[\"RAG_SERVER_URL\"]=\"https://your-rag-server.example.com/v1\"\n",
        "# os.environ[\"INSTRUCT_LLM_BASE_URL\"]=\"https://integrate.api.nvidia.com/v1\"\n",
        "# os.environ[\"NEMOTRON_LLM_BASE_URL\"]=\"https://integrate.api.nvidia.com/v1\"\n",
        "# os.environ[\"EVAL_LLM_BASE_URL\"]=\"https://integrate.api.nvidia.com/v1\"\n",
        "# os.environ[\"RAGAS_LLM_BASE_URL\"]=\"https://integrate.api.nvidia.com/v1\"\n",
        "\n",
        "print(\"Environment overrides set\")\n",
        "key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
        "shown = f\"{key[:6]}...{key[-4:]}\" if len(key) >= 10 else \"***\"\n",
        "print(f\"You have set your NVIDIA_API_KEY (masked): {shown}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ──────────────────────────────────────────────────────────────\n",
        "### QUICK SETUP – point default config to YOUR deployments\n",
        "### ──────────────────────────────────────────────────────────────\n",
        "\n",
        "If you wanted total configuration (recommended), go into configs/eval_config.yml and modify the areas you want and then come back to the notebook\n",
        "\n",
        "This notebook will run the evaluation harness with:\n",
        "- Config file → `configs/eval_config.yml`  \n",
        "- Dataset file → `data/eval_dataset.json`\n",
        "\n",
        "### There are two ways to point the evaluation at a dataset:\n",
        "#### Option A (recommended) : override dataset per run via CLI flag.\n",
        "- nat eval will use this dataset path instead of the config value.\n",
        "- Example: nat eval --config_file \"{config_path}\" --dataset \"{dataset_path}\n",
        "- The next cell below will dynamically resolve and point to the default dataset already present in the repo (e.g., data/eval_dataset.json), so just run it\n",
        "\n",
        "#### Option B : edit eval_config.yml once to set an absolute dataset path.\n",
        "- Set: eval.general.dataset.file_path: /abs/path/to/eval_dataset.json\n",
        "- If this path is wrong/missing, the evaluation will fail.\n",
        "- To run eval: nat eval --config_file \"{config_path}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell will run the evaluation harness with our default config file and dataset path\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    notebook_dir = Path(globals()['_dh'])\n",
        "except Exception:\n",
        "    notebook_dir = Path.cwd()\n",
        "\n",
        "\n",
        "# Project root is the parent of notebooks/ in this repo layout\n",
        "project_root = notebook_dir.parent\n",
        "\n",
        "# Absolute path to the other config file\n",
        "eval_config_path = (project_root / \"configs\" / \"eval_config.yml\").resolve()\n",
        "dataset_path = (project_root / \"data\" / \"eval_dataset.json\").resolve()\n",
        "\n",
        "# Fail early if missing\n",
        "assert eval_config_path.is_file(), f\"Config file not found at: {eval_config_path}\"\n",
        "assert dataset_path.exists(), f\"Dataset path does not exist: {dataset_path}\"\n",
        "\n",
        "# Note: A single full end to end workflow and evaluation may take up to 15-30 minutes\n",
        "# The workflow & metrics output will be in the ./.tmp/aiq_aira/ directory\n",
        "# Run the workflow with our evaluation harness with our default config file and dataset path\n",
        "!nat eval --config_file \"{eval_config_path}\" --dataset \"{dataset_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Evaluators Overview \n",
        "\n",
        "#### Coverage – Inclusion of key facts/claims from the ground truth. \n",
        "- Does the report capture all key facts from the ground truth?\n",
        "\n",
        "#### Synthesis – Integration, comparison/contrast, and coherence across multiple sources.\n",
        "- Does it integrate multiple sources meaningfully, showing alignment or differences?\n",
        "\n",
        "#### Hallucination – Unsupported or non‑grounded claims in the generated report.\n",
        "- Does the output introduce any unsupported claims?\n",
        "\n",
        "#### Citation Quality – Whether claims are supported by the cited sources and citations are precise.\n",
        "- Are references correctly attributed and verifiable via grounding?\n",
        "\n",
        "#### RAGAS metrics – Context relevance, answer accuracy, and groundedness of responses.\n",
        "- Do retrieval and factuality hold up across context relevance, answer accuracy, and groundedness?\n",
        "\n",
        "## How they are scored \n",
        "\n",
        "### Coverage\n",
        "- What it tests: Inclusion of key facts/claims extracted from the ground truth.\n",
        "\n",
        "- Method: Single template evaluation.\n",
        "\n",
        "- Scale: 0 (not covered) to 1 (covered).\n",
        "\n",
        "### Synthesis\n",
        "What it tests: Ability to integrate information from multiple sources, compare, contrast, and draw coherent conclusions.\n",
        "\n",
        "- Method: Dual template evaluation; average then normalize to 0–1.\n",
        "\n",
        "- Scale: 0 (pure extraction) to 4 (expert synthesis), averaged and mapped to 0–1.\n",
        "\n",
        "### Hallucination\n",
        "What it tests: Unsupported or non-grounded claims in the generated report.\n",
        "\n",
        "- Method: Dual template evaluation with two prompts; average the two scores.\n",
        "\n",
        "- Scale: 0 (no hallucination) to 1 (hallucination detected).\n",
        "\n",
        "\n",
        "### Citation quality\n",
        "- What it tests: Whether claims are supported by the cited sources and citations are precise.\n",
        "\n",
        "- Method: Uses RAGAS ResponseGroundedness to verify support; computes precision, recall, and F1 with a 0.5 validity threshold.\n",
        "\n",
        "- Score: Final F1 between 0 and 1.\n",
        "\n",
        "### RAGAS integration\n",
        "- Context relevance: Are retrieved contexts pertinent to the query? Scored via two LLM-judge prompts, normalized to 0–1.\n",
        "\n",
        "- Answer accuracy: Agreement with ground-truth answer using dual-judge scoring mapped to 0–1.\n",
        "\n",
        "- Groundedness: Are response claims supported by retrieved contexts? Dual-judge scoring normalized to 0–1.\n",
        "\n",
        "Notes on scoring mechanics\n",
        "Dual-template metrics average two independent LLM-as-a-judge ratings after mapping to the 0–1 interval.\n",
        "\n",
        "Where native scales differ (e.g., 0–2 or 0–4), scores are normalized to 0–1 for comparability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example Output Files:\n",
        "For reference on what to expect from a full end-to-end evaluation, see the [example workflow output files - pointing to citation f1 but all files live in that directory](../docs/example-workflow-output/citation_f1_output.json) which demonstrate the typical structure and content of evaluation results. If on juptyer, reference docs/example-workflow-output/ for the example output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If you were interested in creating your own custom evaluator the instructions are in the [Evaluate.md](../docs/evaluate.md) under the section title \"Implementing a Custom Evaluator\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip show aiq-aira --version"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
