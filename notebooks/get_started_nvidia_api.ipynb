{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69ebdc3-af82-4ecb-88aa-1ee8c61e83f4",
   "metadata": {},
   "source": [
    "# Get Started With AI-Q NVIDIA Research Assistant Blueprint Using NVIDIA API\n",
    "\n",
    "This notebook helps you get started with the [AI-Q Research Assistant](https://build.nvidia.com/nvidia/aiq).\n",
    "\n",
    "\n",
    "## Prerequisites \n",
    "\n",
    "- This blueprint depends on the [NVIDIA RAG Blueprint](https://github.com/NVIDIA-AI-Blueprints/rag). This deployment guide starts by deploying RAG using docker compose, but you should refer to the [RAG Blueprint documentation](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/quickstart.md) for full details. \n",
    "\n",
    "- Docker Compose\n",
    "\n",
    "- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n",
    "\n",
    "- (Optional) This blueprint supports Tavily web search to supplement data from RAG. A Tavily API key can be supplied to enable this function. \n",
    "\n",
    "- [NVIDIA API Key](https://build.nvidia.com) This notebook uses NVIDIA NIM microservices hosted on build.nvidia.com. To deploy the NIM microservices locally, follow the [getting started deployment guide](../docs/get-started/get-started-docker-compose.md).\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "This notebook uses NVIDIA NIM microservices hosted on build.nvidia.com for the majority of the services that require GPUs. \n",
    "\n",
    "To run this notebook requires:\n",
    "-  1xL40S or comparable\n",
    "-  50GB of disk space\n",
    "-  16 CPUs\n",
    "\n",
    "### NVIDIA NIM Microservices\n",
    "\n",
    "Access  NVIDIA NIM microservices including:   \n",
    "- NemoRetriever  \n",
    "  - Page Elements  \n",
    "  - Table Structure  \n",
    "  - Graphic Elements  \n",
    "  - Paddle OCR   \n",
    "- Llama Instruct 3.3 70B  \n",
    "- Llama Nemotron 3.3 Super 49B  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec310ee9-9e67-4c9c-87e8-dfc74305b5f4",
   "metadata": {},
   "source": [
    "## Step 1: Deploy the RAG Blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e70bde-5052-4f32-881a-6d634e2d3091",
   "metadata": {},
   "source": [
    "See the NVIDIA RAG blueprint documentation for full details. This notebook will use docker compose to deploy the RAG blueprint with *hosted NVIDIA NIM microservices*. Start by setting the appropriate environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1fbb8e53-b8e5-4c91-8080-11507edf6a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To pull images required by the blueprint from NGC, you must first authenticate Docker with nvcr.io.\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "NVIDIA_API_KEY = \"nvapi-your-api-key\"\n",
    "os.environ['NVIDIA_API_KEY'] = NVIDIA_API_KEY\n",
    "os.environ['NGC_API_KEY'] = NVIDIA_API_KEY\n",
    "\n",
    "cmd = f\"echo {NVIDIA_API_KEY} | docker login nvcr.io -u '$oauthtoken' --password-stdin\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd40603-b3bd-4a2b-a92f-4e5023ac7c37",
   "metadata": {},
   "source": [
    "Next, clone the NVIDIA RAG blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82777f6-df62-4f0d-a27e-70b9e2731287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clone the github repository\n",
    "!git clone https://github.com/NVIDIA-AI-Blueprints/rag.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aae4e3",
   "metadata": {},
   "source": [
    "Add the necessary environment variables so that the RAG deployment will use hosted NVIDIA NIM microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd06b03-eefd-47ba-af66-19f669765c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the endpoint urls of the NIMs\n",
    "os.environ[\"APP_LLM_MODELNAME\"] = \"nvidia/llama-3.3-nemotron-super-49b-v1\"\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "os.environ[\"APP_RANKING_MODELNAME\"] = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_RANKING_SERVERURL\"] = \"\"\n",
    "os.environ[\"EMBEDDING_NIM_ENDPOINT\"] = \"https://integrate.api.nvidia.com/v1\"\n",
    "os.environ[\"PADDLE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "os.environ[\"PADDLE_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\"\n",
    "\n",
    "#Disable re-ranking\n",
    "os.environ[\"ENABLE_RERANKER\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d829227d",
   "metadata": {},
   "source": [
    "Deploy the NVIDIA RAG blueprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f923aa-e17b-4c82-82cd-06d9ee65d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the vector db containers from the repo root.\n",
    "!docker compose -f rag/deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3378a-adda-48d5-98b8-36eb2d3b5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the ingestion containers from the repo root. This pulls the prebuilt containers from NGC and deploys it on your system.\n",
    "!docker compose -f rag/deploy/compose/docker-compose-ingestor-server.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6d1d44-3a84-4982-aa5b-0c07ac8abd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the rag containers from the repo root. This pulls the prebuilt containers from NGC and deploys it on your system.\n",
    "!docker compose -f rag/deploy/compose/docker-compose-rag-server.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49426d1a",
   "metadata": {},
   "source": [
    "Confirm all of the containers are running successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc0d5a7-317e-49f9-98bf-a3b90c28b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirm all the below mentioned containers are running.\n",
    "import subprocess\n",
    "\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f88bc",
   "metadata": {},
   "source": [
    "The outputs should look like this: \n",
    "\n",
    "| Container ID | Name | Status |\n",
    "|-------------|------|--------|\n",
    "| bb4f15c42376 | rag-server | Up 2 hours |\n",
    "| 6eb5373d0318 | compose-nv-ingest-ms-runtime-1 | Up 2 hours (healthy) |\n",
    "| 8e53f676486e | ingestor-server | Up 2 hours |\n",
    "| 355f3317a73a | milvus-standalone | Up 2 hours |\n",
    "| b6620d59d4d3 | milvus-minio | Up 2 hours (healthy) |\n",
    "| 0c266aaa1fb1 | milvus-etcd | Up 2 hours (healthy) |\n",
    "| af09adfad86b | rag-playground | Up 2 hours |\n",
    "| d4b7399ab07e | compose-redis-1 | Up 2 hours |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5bd8",
   "metadata": {},
   "source": [
    "At this point, you should be able to access the NVIDIA RAG frontend web application by visiting `http://localhost:8090`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219490a",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-success\\\">\n",
    "    <b>Tip:</b> If you are running this notebook on brev, you will need to make the port for the RAG playground accessible. On the settings page for your machine, navigate to \"Using Ports\", enter \"8090\", click \"Expose Port\", and then click \"I accept\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be23ed3d",
   "metadata": {},
   "source": [
    "To test the RAG deployment:\n",
    "- Navigate to the RAG frontend web application exposed on port 8090.\n",
    "- On the left sidebar, click \"New Collection\".\n",
    "- Select a PDF to upload. We recommend starting with the file `notebooks/simple.pdf` included in the blueprint repository.\n",
    "- After the collection is created and the file is uploaded, select the collection by clicking on it in the left sidebar. \n",
    "- Ask a question in the chat like \"What is the title?\". Confirm that a response is given.\n",
    "\n",
    "*If any of these steps fail, please consult the NVIDIA RAG blueprint [troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/troubleshooting.md) and the [AI-Q Research Assistant troubleshooting guide](../docs/troubleshooting.md) prior to proceeding further*. For problems creating a collection or uploading a file, you can view the logs of the ingestor-server by running `docker logs ingestor-server`. For problems asking a question, you can view the logs of the rag-server by running `docker logs rag-server`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce666f3-2364-42ee-90b5-5bd506f94a3e",
   "metadata": {},
   "source": [
    "## Step 2: Deploy AI-Q NVIDIA  Research Assistant\n",
    "\n",
    "This NVIDIA blueprint allows you to create an AI-Q Research Assistant using NVIDIA AI-Q Toolkit, powered by NVIDIA NIM microservices.\n",
    "\n",
    "The research assistant allows you to:\n",
    "- Provide a desired report structure and topic\n",
    "- Provide human in the loop feedback on a research plan\n",
    "- Perform parallel research of both unstructured on-premise data and web sources\n",
    "- Update the draft report using Q&A \n",
    "- Q&A with the final report for further understanding\n",
    "- View sources from both RAG and web search\n",
    "\n",
    "The blueprint consists of a frontend web interface and a backend API service. To deploy AI-Q Research Assistant, follow the steps below in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852e531-95f8-43f3-925f-b6afc5371e67",
   "metadata": {},
   "source": [
    "1. Clone the Git repository ai-research-assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99aa66b-c0ba-406a-aac8-8c7d84182972",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!git clone https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant.git\n",
    "%cd ai-research-assistant/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3064059",
   "metadata": {},
   "source": [
    "2. Set the necessary environment variables for the service to use hosted NVIDIA NIM microservices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40408efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AIRA_HOSTED_NIMS\"] = \"true\"\n",
    "\n",
    "# optional, if you want to use web search\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"tavily-api-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef117450",
   "metadata": {},
   "source": [
    "3. Deploy the AI-Q Research Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f495d3e2-2cf3-42f0-945b-08ebf18a287c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To deploy the AI-Q Research Assistant run:\n",
    "!docker compose -f deploy/compose/docker-compose.yaml --profile aira up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc648402",
   "metadata": {},
   "source": [
    "Confirm the services have started successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6b1d66",
   "metadata": {},
   "source": [
    "In addition to the RAG services from step 1, you should now also see:  \n",
    "- `aira-backend`  \n",
    "- `aira-frontend`  \n",
    "- `aira-nginx`  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db0644d",
   "metadata": {},
   "source": [
    "You can access the AI-Q Research Assistant frontend web application at `http://<your-server-ip>:3001`. The backend API documentation at `http://<your-server-ip>:8051/docs`. **If any of the services failed to start, refer to the troubleshooting guide in the docs folder**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d73009",
   "metadata": {},
   "source": [
    "<div class=\\\"alert alert-block alert-success\\\">\n",
    "    <b>Tip:</b> If you are running this notebook on brev, you will need to make the ports for the AI-Q Research Assistant demo web frontend accessible. On the settings page for your machine, navigate to \"Using Ports\", enter \"3001\", click \"Expose Port\", and then click \"I accept\". To view the backend REST APIs, repeat these steps for port \"8051\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02acbc2e",
   "metadata": {},
   "source": [
    "## Step 3: Upload Default Collections\n",
    "The demo web application includes two default report prompts. To support these prompts, the blueprint includes two example datasets. In this section we will upload the default datasets using a bulk upload helper. You can also upload your own files through the web interface.\n",
    "\n",
    "Start by running the Docker upload utility. **Note: this command can take upwards of 30 minutes to execute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run \\\n",
    "  -e RAG_INGEST_URL=http://ingestor-server:8082/v1 \\\n",
    "  -e PYTHONUNBUFFERED=1 \\\n",
    "  -v /tmp:/tmp-data \\\n",
    "  --network nvidia-rag \\\n",
    "  nvcr.io/nvidia/blueprint/aira-load-files:v1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27090de3",
   "metadata": {},
   "source": [
    "At the end of the command, you should see a list of documents successfully uploaded for both the Financial_Dataset and the Biomedical_Dataset. You can also confirm the datasets were uploaded by visiting the web frontend and clicking on \"Collections\" in the left sidebar.\n",
    "\n",
    "If any of the file upload steps failed, consult the [NVIDIA RAG blueprint troubleshooting guide](https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/troubleshooting.md) and the [AI-Q Research Assistant troubleshooting guide](../docs/troubleshooting.md) prior to proceeding further. You can check the logs of the ingestor-server by running `docker logs ingestor-server` and the ingestion process by running `docker logs compose-nv-ingest-ms-runtime-1`.\n",
    "\n",
    "**Note: if you see 429 errors in the logs for the compose-nv-ingest-ms-runtime-1 service log it suggests a temporary error. You can re-run the file upload command multiple times, each time the process will pick up where it left off, uploading any documents that failed due to this error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c342b",
   "metadata": {},
   "source": [
    "## Step 4: Use the AI-Q Research Assistant\n",
    "\n",
    "Follow the instructions in the [demo walkthrough](../demo/README.md) to explore the AI-Q Research Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3a9c3",
   "metadata": {},
   "source": [
    "## Step 5: Stop Services\n",
    "\n",
    "To stop all services, run the following commands:\n",
    "\n",
    "1. Stop the AI-Q Research Assistant services:\n",
    "```bash\n",
    "docker compose -f deploy/compose/docker-compose.yaml --profile aira down\n",
    "```\n",
    "\n",
    "2. Stop the RAG services:\n",
    "```bash\n",
    "docker compose -f rag/deploy/compose/docker-compose-rag-server.yaml down\n",
    "docker compose -f rag/deploy/compose/docker-compose-ingestor-server.yaml down\n",
    "docker compose -f rag/deploy/compose/vectordb.yaml down\n",
    "```\n",
    "\n",
    "3. Remove the cache directories:\n",
    "```bash\n",
    "rm -rf rag/deploy/compose/volumes\n",
    "```\n",
    "\n",
    "To verify all services have been stopped, run:\n",
    "```bash\n",
    "docker ps\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
