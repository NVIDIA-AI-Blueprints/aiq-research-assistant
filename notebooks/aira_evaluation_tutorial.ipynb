{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AI-Q AIRA Evaluation Suite Tutorial\n",
        "\n",
        "This notebook provides a comprehensive walkthrough of the AI-Q AIRA Evaluation Suite, demonstrating how to evaluate AI-generated research reports with automatic dataset preprocessing and comprehensive quality metrics.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "- How to set up the AIRA evaluation framework\n",
        "- Creating and preprocessing evaluation datasets  \n",
        "- Configuring and running evaluations\n",
        "- Understanding evaluation metrics and results\n",
        "- Customizing evaluations for your use case\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "Before starting, ensure you have:\n",
        "- Python 3.12+\n",
        "- NVIDIA API Key (from [build.nvidia.com](https://build.nvidia.com))\n",
        "- (Optional) Tavily API Key for web search capabilities\n",
        "- Access to a RAG server endpoint (if running full workflow)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Check Python version\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "\n",
        "# Verify we're in the right directory structure (notebook should be in notebooks/ subdirectory)\n",
        "if not Path(\"../pyproject.toml\").exists():\n",
        "    print(\"Please ensure this notebook is in the notebooks/ directory of the repository\")\n",
        "    print(\"    The '../pyproject.toml' file should be accessible from here.\")\n",
        "    print(\"    Current structure should be: repository_root/notebooks/this_notebook.ipynb\")\n",
        "else:\n",
        "    print(\"Directory structure verified - pyproject.toml found in parent directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install AIRA package directly into the current Python environment\n",
        "print(\"Installing directly into current Python environment...\")\n",
        "print(f\"Using Python: {sys.executable}\")\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Install directly using the current Python interpreter\n",
        "result = subprocess.run([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\", \"-e\", \"..\"\n",
        "], capture_output=True, text=True)\n",
        "\n",
        "print(f\"\\nInstallation result:\")\n",
        "if result.returncode == 0:\n",
        "    print(\"Installation successful!\")\n",
        "    if result.stdout:\n",
        "        print(\"Output:\", result.stdout.strip())\n",
        "else:\n",
        "    print(\"Installation failed!\")\n",
        "    print(\"Error:\", result.stderr)\n",
        "\n",
        "# Test import immediately\n",
        "print(f\"\\nTesting import...\")\n",
        "try:\n",
        "    # Clear any cached modules to get fresh import\n",
        "    modules_to_clear = [m for m in sys.modules.keys() if m.startswith('aiq')]\n",
        "    for module in modules_to_clear:\n",
        "        del sys.modules[module]\n",
        "    \n",
        "    import aiq_aira\n",
        "    print(\"SUCCESS: aiq_aira imported!\")\n",
        "    \n",
        "    # Test CLI command availability  \n",
        "    try:\n",
        "        result_test = subprocess.run([sys.executable, \"-m\", \"aiq_aira.cli\", \"--help\"], \n",
        "                                   capture_output=True, text=True, timeout=5)\n",
        "        if \"aiq eval\" in result_test.stdout:\n",
        "            print(\"SUCCESS: CLI commands available!\")\n",
        "        else:\n",
        "            print(\"CLI might need additional setup\")\n",
        "    except:\n",
        "        print(\"ℹCLI test skipped\")\n",
        "    \n",
        "    print(f\"\\nREADY TO GO!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"Import still failing: {e}\")\n",
        "    print(\"\\nFallback: Using manual path fix...\")\n",
        "    \n",
        "    # Fallback to manual path\n",
        "    aira_src_path = str(Path(\"../aira/src\").resolve())\n",
        "    if aira_src_path not in sys.path:\n",
        "        sys.path.insert(0, aira_src_path)\n",
        "        \n",
        "    try:\n",
        "        import aiq_aira\n",
        "        print(\"SUCCESS with manual path fix!\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"Still failing: {e2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Set Your API Keys\n",
        "\n",
        "Before running the evaluation, you need to set your NVIDIA API key. This is **required** for the evaluation to work.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your API keys here - REPLACE WITH YOUR ACTUAL KEYS!\n",
        "os.environ[\"NVIDIA_API_KEY\"] = \"NVIDIA_API_KEY\"  # Required: Get from build.nvidia.com\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"tvly-YOUR_KEY_HERE\"   # Optional: For web search functionality\n",
        "\n",
        "# Verify environment variables are set\n",
        "api_keys_set = True\n",
        "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-YOUR\"):\n",
        "    print(\"Please set your actual NVIDIA_API_KEY above!\")\n",
        "    print(\"   Get your free key from: https://build.nvidia.com\")\n",
        "    api_keys_set = False\n",
        "else:\n",
        "    print(\"NVIDIA_API_KEY is set\")\n",
        "\n",
        "if os.environ.get(\"TAVILY_API_KEY\", \"\").startswith(\"tvly-YOUR\"):\n",
        "    print(\"TAVILY_API_KEY not set (optional - web search will be disabled)\")\n",
        "else:\n",
        "    print(\"TAVILY_API_KEY is set\")\n",
        "\n",
        "if not api_keys_set:\n",
        "    print(\"\\nPlease set your API keys above before continuing!\")\n",
        "    print(\"   The evaluation will fail without a valid NVIDIA_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Quick Start - Run a Basic Evaluation\n",
        "\n",
        "Let's run a quick evaluation to test everything is working. We'll use the default dataset and configuration included in the repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# QUICK SETUP – point default config to YOUR deployments\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "\n",
        "# If you wanted total configuration (recommended), go into configs/eval_config.yml and modify the all areas and then come back to the notebook\n",
        "\n",
        "# There are two ways to set the dataset path to work with nat eval in this notebook:\n",
        "\n",
        "# Option A (recommended): edit eval_config.yml once to set an absolute dataset path.\n",
        "# Set: eval.general.dataset.file_path: /abs/path/to/eval_dataset.json\n",
        "# If this path is wrong/missing, the evaluation will fail.\n",
        "\n",
        "# Option B (recommended): override dataset per run via CLI flag.\n",
        "# nat eval will use this dataset path instead of the config value.\n",
        "# Example:\n",
        "# !nat eval --config_file \"{config_path}\" --dataset \"{dataset_path}\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Hosted Nemotron Backend (for reasoning)\n",
        "os.environ[\"NEMOTRON_LLM_BASE_URL\"]  = \"http://nim-llm-ms:8000/v1\"\n",
        "# RAG server for generate_summary / artifact_qa\n",
        "os.environ[\"RAG_SERVER_URL\"]         = \"http://rag-server:8081/v1\"\n",
        "\n",
        "os.environ[\"EVAL_LLM_BASE_URL\"]         = \"https://integrate.api.nvidia.com/v1\"\n",
        "\n",
        "\n",
        "\n",
        "# NVIDIA hosted models (leave as-is if you still want integrate.api)\n",
        "# os.environ[\"RAGAS_LLM_BASE_URL\"]    = \"https://integrate.api.nvidia.com/v1\"\n",
        "\n",
        "\n",
        "print(\"Environment overrides set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will run the evaluation with the default config file that exists in the configs folder called eval_config.yml\n",
        "# The workflow & metrics output will be in the notebooks/.tmp/aiq_aira_similarity folder.\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    notebook_dir = Path(globals()['_dh'])\n",
        "except Exception:\n",
        "    notebook_dir = Path.cwd()\n",
        "\n",
        "\n",
        "# Project root is the parent of notebooks/ in this repo layout\n",
        "project_root = notebook_dir.parent\n",
        "\n",
        "# Absolute path to the other config file\n",
        "eval_config_path = (project_root / \"configs\" / \"eval_config.yml\").resolve()\n",
        "\n",
        "# Fail early if missing\n",
        "assert eval_config_path.is_file(), f\"Config file not found at: {eval_config_path}\"\n",
        "\n",
        "# Option A: Run nat with our evaluation harness if you set the dataset path in the config file\n",
        "!nat eval --config_file \"{eval_config_path}\"\n",
        "\n",
        "# Option B: Run nat with our evaluation harness if you want to set the dataset path in the command line (uncomment this line and comment the line above)\n",
        "# !nat eval --config_file \"{eval_config_path}\" --dataset \"{dataset_path}\"\n",
        "\n",
        "# There are a lot of logs, so it might be worth redirecting to a file so we can see the output easier just use this command\n",
        "# !nat eval --config_file \"{eval_config_path}\" > output.txt 2>&1 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip show aiq-aira --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optional: Let's get an CUSTOM EVALUATOR set up together!\n",
        "By running the cells below, the steps to create a custom evaluator are as follows:\n",
        "1. Create the file with the evaluators code (we'll be making a pretty basic similarity checker)\n",
        "2. Add it into the evaluator_register.py file\n",
        "3. Then reinstall the package to register it\n",
        "4. Then create a custom config file to only run that evaluator (Be sure to change the anything in the code below if you're using any endpoints and etc. You could change it in the config file itself after creation)\n",
        "5. Run the evaluation with the new custom evaluator\n",
        "6. You will see the similarity_evaluator_output.json under notebooks/.tmp/aiq_aira_similarity/similarity_evaluator_output.json\n",
        "7. This is a very simple example you should repeat steps 1-6 to add more evaluators on your own accord!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ../aira/src/aiq_aira/eval/evaluators/similarity_evaluator.py\n",
        "# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "\"\"\"\n",
        "Custom Similarity Evaluator - Demo evaluator for tutorial purposes.\n",
        "Computes cosine similarity between generated report and ground truth.\n",
        "\"\"\"\n",
        "import asyncio\n",
        "import logging\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from aiq.data_models.component_ref import LLMRef\n",
        "from aiq.data_models.evaluator import EvaluatorBaseConfig\n",
        "from aiq.eval.evaluator.evaluator_model import EvalInput, EvalInputItem, EvalOutput, EvalOutputItem\n",
        "from pydantic import Field\n",
        "\n",
        "from aiq_aira.eval.schema import AIResearcherEvalOutput\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class SimilarityEvaluatorConfig(EvaluatorBaseConfig, name=\"similarity_evaluator\"):\n",
        "    \"\"\"Configuration for similarity evaluator.\"\"\"\n",
        "    model_name: str = Field(\"all-MiniLM-L6-v2\", description=\"SentenceTransformer model to use\")\n",
        "\n",
        "class SimilarityEvaluator:\n",
        "    \"\"\"Evaluator that computes cosine similarity between generated and ground truth text.\"\"\"\n",
        "    \n",
        "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\", output_dir: str = None):\n",
        "        self.model_name = model_name\n",
        "        self.output_dir = output_dir\n",
        "        self._model = None\n",
        "    \n",
        "    def _get_model(self):\n",
        "        \"\"\"Lazy load the sentence transformer model\"\"\"\n",
        "        if self._model is None:\n",
        "            self._model = SentenceTransformer(self.model_name)\n",
        "        return self._model\n",
        "    \n",
        "    async def evaluate_item(self, item: EvalInputItem) -> EvalOutputItem:\n",
        "        \"\"\"Evaluate a single item - FIXED to use correct data structure\"\"\"\n",
        "        try:\n",
        "            # Follow the same pattern as other evaluators\n",
        "            if item.output_obj == \"\":\n",
        "                # If workflow is skipped, input_obj contains the data source\n",
        "                item.output_obj = item.input_obj\n",
        "            \n",
        "            # Parse the data using the AIRA schema (same as other evaluators)\n",
        "            data_source = AIResearcherEvalOutput.model_validate_json(item.output_obj)\n",
        "            logger.info(f\"Processing similarity evaluation for item {data_source.id}\")\n",
        "            \n",
        "            # Extract the generated report and ground truth (following other evaluators)\n",
        "            generated = data_source.finalized_summary\n",
        "            ground_truth = data_source.ground_truth\n",
        "            \n",
        "            if not generated or not generated.strip():\n",
        "                return EvalOutputItem(\n",
        "                    id=item.id, \n",
        "                    score=0.0, \n",
        "                    reasoning={\"error\": \"Generated report (finalized_summary) is empty\"}\n",
        "                )\n",
        "            \n",
        "            if not ground_truth or not ground_truth.strip():\n",
        "                return EvalOutputItem(\n",
        "                    id=item.id, \n",
        "                    score=0.0, \n",
        "                    reasoning={\"error\": \"Ground truth is empty\"}\n",
        "                )\n",
        "            \n",
        "            # Compute cosine similarity\n",
        "            model = self._get_model()\n",
        "            embeddings = model.encode([generated, ground_truth])\n",
        "            similarity = float(util.cos_sim(embeddings[0], embeddings[1]))\n",
        "            \n",
        "            logger.info(f\"Item {data_source.id}: Similarity score: {similarity:.3f}\")\n",
        "            \n",
        "            return EvalOutputItem(\n",
        "                id=item.id,\n",
        "                score=similarity,\n",
        "                reasoning={\n",
        "                    \"similarity_score\": similarity,\n",
        "                    \"generated_length\": len(generated),\n",
        "                    \"ground_truth_length\": len(ground_truth),\n",
        "                    \"model_used\": self.model_name\n",
        "                }\n",
        "            )\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"Similarity evaluation failed for item {item.id}: {str(e)}\")\n",
        "            return EvalOutputItem(\n",
        "                id=item.id, \n",
        "                score=0.0, \n",
        "                reasoning={\"error\": f\"Similarity evaluation failed: {str(e)}\"}\n",
        "            )\n",
        "    \n",
        "    async def evaluate(self, eval_input: EvalInput) -> EvalOutput:\n",
        "        \"\"\"Evaluate all items\"\"\"\n",
        "        eval_output_items = []\n",
        "        for item in eval_input.eval_input_items:\n",
        "            result = await self.evaluate_item(item)\n",
        "            eval_output_items.append(result)\n",
        "        \n",
        "        # Calculate average score\n",
        "        scores = [item.score for item in eval_output_items if item.score is not None]\n",
        "        avg_score = sum(scores) / len(scores) if scores else 0.0\n",
        "        \n",
        "        logger.info(f\"Similarity evaluator completed: {len(scores)} valid scores, average: {avg_score:.3f}\")\n",
        "        \n",
        "        return EvalOutput(average_score=avg_score, eval_output_items=eval_output_items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "reg_path = Path(\"../aira/src/aiq_aira/eval/evaluator_register.py\")\n",
        "reg_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "base_imports = (\n",
        "    \"from aiq.builder.builder import EvalBuilder\\n\"\n",
        "    \"from aiq.builder.evaluator import EvaluatorInfo\\n\"\n",
        "    \"from aiq.cli.register_workflow import register_evaluator\\n\"\n",
        ")\n",
        "\n",
        "text = reg_path.read_text(encoding=\"utf-8\") if reg_path.exists() else \"\"\n",
        "if \"register_evaluator\" not in text or \"EvaluatorInfo\" not in text:\n",
        "    reg_path.write_text(base_imports + \"\\n\", encoding=\"utf-8\")\n",
        "    print(\"Wrote base imports to:\", reg_path.resolve())\n",
        "else:\n",
        "    print(\"Imports already present:\", reg_path.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile -a ../aira/src/aiq_aira/eval/evaluator_register.py\n",
        "# Custom similarity evaluator registration\n",
        "from aiq_aira.eval.evaluators.similarity_evaluator import SimilarityEvaluator, SimilarityEvaluatorConfig\n",
        "\n",
        "@register_evaluator(config_type=SimilarityEvaluatorConfig)\n",
        "async def register_similarity_evaluator(config: SimilarityEvaluatorConfig, builder: EvalBuilder):\n",
        "    \"\"\"Register the similarity evaluator.\"\"\"\n",
        "    evaluator = SimilarityEvaluator(\n",
        "        model_name=config.model_name,\n",
        "        output_dir=builder.eval_general_config.output_dir,\n",
        "    )\n",
        "    yield EvaluatorInfo(\n",
        "        config=config,\n",
        "        evaluate_fn=evaluator.evaluate,\n",
        "        description=\"Cosine Similarity Evaluator\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, sys\n",
        "\n",
        "# 1) Fix tokenizers/transformers compatibility before loading sentence_transformers\n",
        "print(\"Ensuring transformers/tokenizers compatibility (tokenizers>=0.21,<0.22 + transformers latest)...\")\n",
        "fix_cmds = [\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"tokenizers>=0.21,<0.22\"],\n",
        "    [sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"transformers\"],\n",
        "]\n",
        "for cmd in fix_cmds:\n",
        "    res = subprocess.run(cmd, capture_output=True, text=True)\n",
        "    if res.returncode != 0:\n",
        "        print(\"Dependency fix step failed:\")\n",
        "        print(res.stderr or res.stdout)\n",
        "        # Do not exit here; continue to reinstall so the error is visible if it persists.\n",
        "\n",
        "# 2) Reinstall the package\n",
        "print(\"Reinstalling package with our new similarity evaluator (clean, no deps, no cache)...\")\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"pip\", \"install\",\n",
        "    \"-e\", \"..[dev]\",\n",
        "    \"--force-reinstall\",\n",
        "    \"--no-deps\",\n",
        "    \"--no-cache-dir\",\n",
        "    \"--quiet\",\n",
        "]\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "if result.returncode == 0:\n",
        "    print(\"Package reinstalled successfully!\")\n",
        "    print(\"Your new similarity evaluator should now work correctly\")\n",
        "else:\n",
        "    print(\"Reinstall failed:\")\n",
        "    print(result.stderr or result.stdout)\n",
        "\n",
        "print(\"Note: If imports were already attempted in this kernel, a kernel restart may be required so transformers/tokenizers are re-imported cleanly.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Run this to see the evaluators available, if you created the custom evaluator with the cells above, you should see the similarity_evaluator here!\n",
        "!nat info components -t evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "cfg_path = Path(\"../configs/eval_config_with_similarity.yml\")\n",
        "cfg_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "print(\"Will write to:\", cfg_path.resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile ../configs/eval_config_with_similarity.yml\n",
        "# SPDX-FileCopyrightText: Copyright (c) 2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n",
        "# SPDX-License-Identifier: Apache-2.0\n",
        "\n",
        "general:\n",
        "  use_uvloop: true\n",
        "  front_end:\n",
        "    _type: fastapi\n",
        "    endpoints:\n",
        "      - path: /generate_query\n",
        "        method: POST\n",
        "        description: Creates the query\n",
        "        function_name: generate_query\n",
        "      - path: /generate_summary\n",
        "        method: POST\n",
        "        description: Generates the summary\n",
        "        function_name: generate_summary\n",
        "      - path: /artifact_qa\n",
        "        method: POST\n",
        "        description: Q/A or chat about a previously generated artifact\n",
        "        function_name: artifact_qa\n",
        "      - path: /aiqhealth\n",
        "        method: GET\n",
        "        description: Health check for the AIQ AIRA service\n",
        "        function_name: health_check\n",
        "      - path: /default_collections\n",
        "        method: GET\n",
        "        description: Get the default collections\n",
        "        function_name: default_collections\n",
        "      - path: /default_prompt\n",
        "        method: GET\n",
        "        description: Get the default prompt\n",
        "        function_name: default_prompt\n",
        "\n",
        "  telemetry:\n",
        "    logging:\n",
        "      console:\n",
        "        _type: console\n",
        "        level: DEBUG\n",
        "    tracing:\n",
        "      weave:\n",
        "        _type: weave\n",
        "        project: \"NAT-BP-Project-Default\"\n",
        "\n",
        "llms:\n",
        "  instruct_llm:\n",
        "    _type: nim\n",
        "    model_name: meta/llama-3.3-70b-instruct\n",
        "    temperature: 0.0\n",
        "    base_url: ${INSTRUCT_LLM_BASE_URL:-http://aira-instruct-llm:8000/v1}\n",
        "    api_key: not-needed\n",
        "  \n",
        "  nemotron:\n",
        "    _type: nim\n",
        "    model_name: nvidia/llama-3_3-nemotron-super-49b-v1_5\n",
        "    temperature: 0.5\n",
        "    base_url: ${NEMOTRON_LLM_BASE_URL:-http://nim-llm-ms:8000/v1}\n",
        "    disable_streaming: false\n",
        "    api_key: not-needed\n",
        "    max_tokens: 5000\n",
        "\n",
        "  eval_llm:\n",
        "    _type: nim\n",
        "    model_name: meta/llama-3.1-70b-instruct\n",
        "    temperature: 0.0\n",
        "    base_url: https://integrate.api.nvidia.com/v1\n",
        "    api_key: ${NVIDIA_API_KEY}\n",
        "\n",
        "  ragas_llm:\n",
        "    _type: nim\n",
        "    model_name: nvdev/mistralai/mixtral-8x22b-instruct-v0.1\n",
        "    temperature: 0.0\n",
        "    base_url: https://integrate.api.nvidia.com/v1\n",
        "    api_key: ${NVIDIA_API_KEY}\n",
        "\n",
        "functions:\n",
        "  generate_query:\n",
        "    _type: generate_queries\n",
        "\n",
        "  generate_summary:\n",
        "    _type: generate_summaries\n",
        "    rag_url: http://rag-server:8081/v1\n",
        "\n",
        "  artifact_qa:\n",
        "    _type: artifact_qa\n",
        "    llm_name: instruct_llm\n",
        "    rag_url: http://rag-server:8081/v1\n",
        "    \n",
        "  health_check:\n",
        "    _type: health_check\n",
        "\n",
        "\n",
        "  default_prompt:\n",
        "    _type: default_prompt\n",
        "\n",
        "  default_collections:\n",
        "    _type: default_collections\n",
        "    collections:\n",
        "      - name: \"Biomedical_Dataset\"\n",
        "        topic: \"Biomedical\"\n",
        "        report_organization: \"You are a medical researcher who specializes in cystic fibrosis. Create a report analyzing how CFTR modulators can be used to restore CFTR protein functions. Include a 150-200 word abstract and a methods, results, and discussion section. Format your answer in paragraphs. Consider all (and only) relevant data. Give a factual report with cited sources.\"\n",
        "      - name: \"Financial_Dataset\"\n",
        "        topic: \"Financial\"\n",
        "        report_organization: \"You are a financial analyst who specializes in financial statement analysis. Write a financial report analyzing the 2023 financial performance of Amazon. Identify trends in revenue growth, net income, and total assets. Discuss how these trends affected Amazon's yearly financial performance for 2023. Your output should be organized into a brief introduction, as many sections as necessary to create a comprehensive report, and a conclusion. Format your answer in paragraphs. Use factual sources such as Amazon's quarterly meeting releases for 2023. Cross analyze the sources to draw original and sound conclusions and explain your reasoning for arriving at conclusions. Do not make any false or unverifiable claims. I want a factual report with cited sources.\"\n",
        "\n",
        "workflow:\n",
        "  _type: aira_evaluator_workflow\n",
        "  generator:\n",
        "    _type: full\n",
        "    verbose: true\n",
        "    fact_extraction_llm: meta/llama-3.1-70b-instruct\n",
        "    citation_pairing_llm: mistralai/mixtral-8x22b-instruct-v0.1\n",
        "\n",
        "# Evaluation configuration - ONLY with custom similarity evaluator\n",
        "eval:\n",
        "  general:\n",
        "    output_dir: ./.tmp/aiq_aira_similarity/\n",
        "    cleanup: true\n",
        "\n",
        "    dataset:\n",
        "      _type: json\n",
        "      # Replace with your own dataset path\n",
        "      file_path: /Users/kyzheng/aiq-internal-notebook/data/eval_dataset_processed.json\n",
        "      id_key: id\n",
        "      structure:\n",
        "        disable: true\n",
        "    profiler:\n",
        "      base_metrics: true\n",
        "\n",
        "  evaluators:\n",
        "    # ONLY our custom similarity evaluator - no built-in evaluators\n",
        "    similarity_evaluator:\n",
        "      _type: similarity_evaluator\n",
        "      model_name: all-MiniLM-L6-v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lets test out your custom similarity evaluator\n",
        "custom_config_path = Path(\"../configs/eval_config_with_similarity.yml\").resolve()\n",
        "\n",
        "print(f\"Testing your custom similarity evaluator\")\n",
        "print(f\"Config: {custom_config_path}\")\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "\n",
        "!nat eval --config_file \"{custom_config_path}\"\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
