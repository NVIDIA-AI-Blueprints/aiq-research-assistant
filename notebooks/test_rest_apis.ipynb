{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test REST APIs\n",
    "\n",
    "This notebook provides example commands for interacting with the AI-Q Research Assistant backend APIs. Before using this notebook, ensure you have followed the deployment guide to deploy the NVIDIA RAG blueprint and the AI-Q Reseearch Assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Update the following environment variables with the IP address of the server where you are running these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import subprocess\n",
    "import sys\n",
    "os.environ[\"INFERENCE_ORIGIN\"]=\"http://your-server-ip:3838\"\n",
    "os.environ[\"RAG_BASE_URL\"]= \"http://your-server-ip\"\n",
    "\n",
    "#Helper function to measure the time taken to execute the query\n",
    "import time\n",
    "\n",
    "def run_curl(curl_cmd, max_output_lines=50, save_to_file=None):\n",
    "    print(f\"\\nExecuting: {curl_cmd}\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Run the command\n",
    "    result = subprocess.run(curl_cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    # Handle large outputs\n",
    "    output_lines = result.stdout.strip().split('\\n') if result.stdout.strip() else []\n",
    "    \n",
    "    if save_to_file and result.stdout.strip():\n",
    "        with open(save_to_file, 'w') as f:\n",
    "            f.write(result.stdout.strip())\n",
    "        print(f\"\\n--- Response saved to {save_to_file} ---\")\n",
    "        print(f\"Output contains {len(output_lines)} lines\")\n",
    "        \n",
    "        # Show first few and last few lines\n",
    "        if len(output_lines) > max_output_lines:\n",
    "            print(\"\\n--- First 20 lines ---\")\n",
    "            for line in output_lines[:20]:\n",
    "                print(line)\n",
    "            print(f\"\\n... ({len(output_lines) - 40} lines truncated) ...\")\n",
    "            print(\"\\n--- Last 20 lines ---\")\n",
    "            for line in output_lines[-20:]:\n",
    "                print(line)\n",
    "        else:\n",
    "            print(\"\\n--- Full Response ---\")\n",
    "            print(result.stdout.strip())\n",
    "    else:\n",
    "        print(\"\\n--- Response ---\")\n",
    "        if len(output_lines) > max_output_lines:\n",
    "            print(f\"Output too large ({len(output_lines)} lines). Showing first {max_output_lines} lines:\")\n",
    "            for line in output_lines[:max_output_lines]:\n",
    "                print(line)\n",
    "            print(f\"\\n... ({len(output_lines) - max_output_lines} more lines truncated)\")\n",
    "            print(\"üí° Tip: Use save_to_file parameter to save full output to a file\")\n",
    "        else:\n",
    "            print(result.stdout.strip())\n",
    "\n",
    "    if result.stderr:\n",
    "        print(\"\\n--- Errors ---\")\n",
    "        print(result.stderr.strip())\n",
    "\n",
    "    print(f\"\\n‚è±Ô∏è  Completed in {duration:.2f} seconds\\n\")\n",
    "\n",
    "def run_curl_streaming(curl_cmd, save_to_file=None, show_progress=True):\n",
    "    \"\"\"\n",
    "    Special handler for streaming endpoints that can produce large outputs\n",
    "    \"\"\"\n",
    "    print(f\"\\nExecuting streaming command: {curl_cmd}\")\n",
    "    start = time.time()\n",
    "    \n",
    "    if save_to_file:\n",
    "        # Redirect output to file to avoid IOPub limit\n",
    "        cmd_with_redirect = f\"{curl_cmd} > {save_to_file} 2>&1\"\n",
    "        result = subprocess.run(cmd_with_redirect, shell=True)\n",
    "        \n",
    "        duration = time.time() - start\n",
    "        print(f\"\\n‚úÖ Streaming response saved to {save_to_file}\")\n",
    "        \n",
    "        # Show file size and first few lines\n",
    "        try:\n",
    "            with open(save_to_file, 'r') as f:\n",
    "                content = f.read()\n",
    "                lines = content.split('\\n')\n",
    "                print(f\"üìÑ File size: {len(content)} characters, {len(lines)} lines\")\n",
    "                \n",
    "                if len(lines) > 10:\n",
    "                    print(\"\\n--- First 10 lines of response ---\")\n",
    "                    for line in lines[:10]:\n",
    "                        print(line)\n",
    "                    print(\"... (see full response in file)\")\n",
    "                else:\n",
    "                    print(\"\\n--- Full Response ---\")\n",
    "                    print(content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading saved file: {e}\")\n",
    "            \n",
    "        print(f\"\\n‚è±Ô∏è  Completed in {duration:.2f} seconds\\n\")\n",
    "    else:\n",
    "        # For non-file output, use regular run_curl with truncation\n",
    "        run_curl(curl_cmd, max_output_lines=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Collection\n",
    "\n",
    "The following command creates a collection. For any unsuccessful query executions, check the logs in the following containers for troubleshooting:\n",
    " - `docker logs aira-backend`\n",
    " - `docker logs ingestor-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "run_curl(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/collection \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{{\n",
    "    \"collection_name\": \"test_collection\",\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"embedding_dimension\": 2048,\n",
    "    \"metadata_schema\": []\n",
    "  }}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload a File to the Collection\n",
    "\n",
    "The following command uploads a file to the test collection. For any unsuccessful query executions, check the logs in the following container for troubleshooting:\n",
    "\n",
    "- `docker logs ingestor-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " # Update the directory based on where you have downloaded the notebook\n",
    " \n",
    " run_curl(f\"\"\"curl -X POST {os.environ['INFERENCE_ORIGIN']}/documents \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -F 'documents=@simple.pdf' \\\n",
    "  -F 'data={{\\\"collection_name\\\": \\\"test_collection\\\"}}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Questions About Your PDF to RAG\n",
    "\n",
    "The following endpoint allows you to ask questions about your PDF to RAG. For any errors, check the logs of the container below for troubleshooting:\n",
    "\n",
    "- `docker logs rag-server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " run_curl(f\"\"\"curl -X POST \"{os.environ['RAG_BASE_URL']}:8081/v1/generate\" \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{ \n",
    "            \"messages\": [ \n",
    "                {{ \\\n",
    "                \"role\": \"user\", \n",
    "                \"content\": \"Give me a summary of the topic\" \n",
    "                }} \n",
    "            ], \n",
    "            \"use_knowledge_base\": true, \n",
    "            \"temperature\": 0.2, \n",
    "            \"top_p\": 0.7, \n",
    "            \"max_tokens\": 1024, \n",
    "            \"reranker_top_k\": 10, \n",
    "            \"vdb_top_k\": 100, \n",
    "            \"vdb_endpoint\": \"http://milvus:19530\", \n",
    "            \"collection_name\": \"test_collection\", \n",
    "            \"enable_query_rewriting\": false, \n",
    "            \"enable_reranker\": false, \n",
    "            \"enable_guardrails\": false, \n",
    "            \"enable_citations\": true, \n",
    "            \"stop\": [] \n",
    "            }}'\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Sample Research Plan\n",
    "\n",
    "This command replicates the first part of the AI-Q Research Assistant workflow by creating a research plan given a collection, report topic, and report organization prompt.Troubleshooting Steps:\n",
    "- If no response is given, check the backend logs by running `docker logs aira-backend`.\n",
    "- If a 403 or 404 is given, check that the nemotron API key and model configuration in the aira config.yaml file are correct. Try making a direct shell request to the nemotron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using streaming function to handle potentially large output\n",
    "run_curl_streaming(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/generate_query/stream \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{\n",
    "            \"topic\": \"An awesome report\",\n",
    "            \"report_organization\": \"A comprehensive report with an introduction, body, and conclusion with a witty joke\",\n",
    "            \"num_queries\": 3,\n",
    "            \"llm_name\": \"nemotron\"\n",
    "    }}'\"\"\", save_to_file=\"research_plan.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a Report\n",
    "\n",
    "The following command generates a report based on the topic, report organization, and queries. If you encounter any errors while running it, you can troubleshoot by checking the logs of the following container:\n",
    "- `docker logs aira-backend`\n",
    "\n",
    "*Note that the following command outputs the response.txt file instead of printing the output to the console.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using streaming function to handle large report output\n",
    "run_curl_streaming(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/generate_summary/stream \\\n",
    "        -H 'accept: application/json' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{{\n",
    "        \"topic\": \"An awesome report\",\n",
    "        \"report_organization\": \"A comprehensive report with an introduction, body, and conclusion with a witty joke\",\n",
    "        \"queries\": [\n",
    "            {{\n",
    "            \"query\": \"overview of the topic\",\n",
    "            \"report_section\": \"overview\",\n",
    "            \"rationale\": \"key information\"\n",
    "            }},\n",
    "            {{\n",
    "            \"query\": \"big mac ingredients\",\n",
    "            \"report_section\": \"web search\",\n",
    "            \"rationale\": \"just for fun\"\n",
    "            }}\n",
    "        ],\n",
    "        \"search_web\": true,\n",
    "        \"rag_collection\": \"test_collection\",\n",
    "        \"reflection_count\": 1,\n",
    "        \"llm_name\": \"nemotron\"\n",
    "    }}'\"\"\", save_to_file=\"report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask Q&A\n",
    "\n",
    "The following command replicates the human in the loop interactions available in the web frontend including editing the report plan, the draft report, and doing Q&A. \n",
    "\n",
    "Troubleshooting tips:You can troubleshoot by checking the logs of the container below:\n",
    "- `docker logs aira-backend`\n",
    "\n",
    "The Q&A endpoint relies on the instruct_llm configured in the AI-Q Research Assistant config.yaml file. Verify this configuration and attempt a direct shell command against that llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using streaming function to handle Q&A response\n",
    "run_curl_streaming(f\"\"\"curl -s -X POST {os.environ['INFERENCE_ORIGIN']}/artifact_qa/stream \\\n",
    "        -H \"accept: application/json\" \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d '{{\n",
    "            \"additional_context\": \"\",\n",
    "            \"artifact\": \"\\\\n\\\\n# An extensive report on the topic of the users favorite PDF\",\n",
    "            \"chat_history\": [],\n",
    "            \"question\": \"edit the title to something more professional\",\n",
    "            \"rewrite_mode\": \"entire\",\n",
    "            \"use_internet\": false,\n",
    "            \"rag_collection\": \"test_collection\"\n",
    "        }}'\"\"\", save_to_file=\"qa_response.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
